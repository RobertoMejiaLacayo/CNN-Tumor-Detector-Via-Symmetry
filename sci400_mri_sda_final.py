# -*- coding: utf-8 -*-
"""SCI400_MRI SDA_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RfMaYp7kIxHLT_A27XJQL4qeuF38fJNe
"""

# Install PyTorch 2.1.1 with CUDA 12.1 support
!pip install torch torchvision torchaudio

import os
import cv2
import torch
import torch.nn as nn
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
from torchvision.models import resnet50, ResNet50_Weights
from PIL import Image
import pandas as pd
from google.colab import drive
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Mount Google Drive
drive.mount('/content/drive/')

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define data paths
parent_dir_training = '/content/drive/MyDrive/SCI400/dataset/training'
parent_dir_validation = '/content/drive/MyDrive/SCI400/dataset/validation'
parent_dir_test = '/content/drive/MyDrive/SCI400/dataset/test'

def generate_simple_labels(directories):
    labels_dict = {}

    for parent_dir in directories:
        if not os.path.exists(parent_dir):
            print(f"Directory not found: {parent_dir}")
            continue

        for filename in os.listdir(parent_dir):
            filename_normalized = filename.strip().lower()

            if "normal" in filename_normalized:
                label = 0  # Non-tumor
            else:
                label = 1  # Tumor or other condition

            # Add to the dictionary
            labels_dict[filename] = label

    return labels_dict


parent_dirs = [
    '/content/drive/MyDrive/SCI400/dataset/training',
    '/content/drive/MyDrive/SCI400/dataset/validation',
    '/content/drive/MyDrive/SCI400/dataset/test'
]

labels_dict = generate_simple_labels(parent_dirs)

# MRIDataset Class
class MRIDataset(Dataset):
    def __init__(self, image_dir, labels, transform=None, use_whole_image=False, padding=10):
        self.image_dir = image_dir
        self.transform = transform
        self.use_whole_image = use_whole_image
        self.padding = padding

        # Normalize and clean the labels dictionary (keep file extensions)
        self.labels = {key.strip(): value for key, value in labels.items()}

        # List image filenames in the directory (with extensions)
        self.image_filenames = [
            f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))
        ]
        self.image_filenames = [f.strip() for f in self.image_filenames]  # Strip spaces

        # Check for missing labels (filenames with extensions)
        missing_files = [
            f for f in self.image_filenames if f not in self.labels
        ]
        if missing_files:
            print(f"Warning: {len(missing_files)} files do not have labels!")
            print("Examples of missing files:", missing_files[:10])

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        # Get filename with extension
        original_filename = self.image_filenames[idx]

        # Retrieve label based on full filename (with extension)
        label = self.labels.get(original_filename, 0)  # Default to label 0 if not found

        # Load and preprocess the image
        image_path = os.path.join(self.image_dir, original_filename)
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        image = cv2.resize(image, (224, 224))

        # Use whole image or split into hemispheres
        if self.use_whole_image:
            # Apply transforms to the whole image
            if self.transform:
                image = self.transform(image)
            return image, label
        else:
            # Split into left and right hemispheres
            height, width = image.shape
            midline = width // 2
            left_img = image[:, max(0, midline - self.padding):midline + self.padding]
            right_img = image[:, midline - self.padding:min(width, midline + self.padding)]

            # Apply transforms to each hemisphere
            if self.transform:
                left_img = self.transform(left_img)
                right_img = self.transform(right_img)

            return left_img, right_img, label

# Data augmentation
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Initialize datasets/dataloaders
train_dataset_whole = MRIDataset(parent_dir_training, labels_dict, transform=transform, use_whole_image=True)
train_dataset_hemispheres = MRIDataset(parent_dir_training, labels_dict, transform=transform, use_whole_image=False)

val_dataset_whole = MRIDataset(parent_dir_validation, labels_dict, transform=transform, use_whole_image=True)
val_dataset_hemispheres = MRIDataset(parent_dir_validation, labels_dict, transform=transform, use_whole_image=False)

test_dataset_whole = MRIDataset(parent_dir_test, labels_dict, transform=transform, use_whole_image=True)
test_dataset_hemispheres = MRIDataset(parent_dir_test, labels_dict, transform=transform, use_whole_image=False)

train_loader_whole = DataLoader(train_dataset_whole, batch_size=64, shuffle=True)
train_loader_hemispheres = DataLoader(train_dataset_hemispheres, batch_size=64, shuffle=True)

val_loader_whole = DataLoader(val_dataset_whole, batch_size=64, shuffle=False)
val_loader_hemispheres = DataLoader(val_dataset_hemispheres, batch_size=64, shuffle=False)

test_loader_whole = DataLoader(test_dataset_whole, batch_size=64, shuffle=False)
test_loader_hemispheres = DataLoader(test_dataset_hemispheres, batch_size=64, shuffle=False)

dataloaders_whole = {'train': train_loader_whole, 'val': val_loader_whole}
dataloaders_hemispheres = {'train': train_loader_hemispheres, 'val': val_loader_hemispheres}

# SymmetryResNet Class
class SymmetryResNet(nn.Module):
    def __init__(self):
        super(SymmetryResNet, self).__init__()
        self.resnet = models.resnet50(pretrained=True)

        for param in list(self.resnet.parameters())[:6]:
            param.requires_grad = False

        num_ftrs = self.resnet.fc.in_features
        self.resnet.fc = nn.Identity()
        self.fc = nn.Linear(num_ftrs * 2, 2)

    def forward(self, left_img, right_img):
        left_img = left_img.repeat(1, 3, 1, 1)
        right_img = right_img.repeat(1, 3, 1, 1)
        left_features = self.resnet(left_img)
        right_features = self.resnet(right_img)
        combined = torch.cat((left_features, right_features), dim=1)

        output = self.fc(combined)
        return output

# FineTunedResNet Class
class FineTunedResNet(nn.Module):
    def __init__(self):
        super(FineTunedResNet, self).__init__()
        self.resnet = models.resnet50(pretrained=True)
        num_ftrs = self.resnet.fc.in_features
        self.resnet.fc = nn.Linear(num_ftrs, 2)

    def forward(self, x):
        x = x.repeat(1, 3, 1, 1)
        return self.resnet(x)

# Function to calculate and display metrics
def calculate_metrics(outputs, labels, phase, epoch, model_name):
    probs = torch.sigmoid(outputs)
    preds = (probs > 0.5).int().squeeze()

    preds = preds.cpu().numpy()
    labels = labels.cpu().numpy()

    # Calculate evaluation metrics
    acc = accuracy_score(labels, preds)
    precision = precision_score(labels, preds, average='binary', zero_division=1)
    recall = recall_score(labels, preds, average='binary', zero_division=1)
    f1 = f1_score(labels, preds, average='binary', zero_division=1)
    cm = confusion_matrix(labels, preds)

    # Display confusion matrix
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["NONTUMOR", "TUMOR"], yticklabels=["NONTUMOR", "TUMOR"])
    plt.title(f"{model_name} Confusion Matrix ({phase} - Epoch {epoch+1})")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # Return metrics
    return acc, precision, recall, f1, cm

# Main training loop
def train_and_evaluate(model, dataloaders, criterion, optimizer, num_epochs=10, model_name="Model"):
    metrics = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": [],
               "train_precision": [], "val_precision": [], "train_recall": [], "val_recall": [],
               "train_f1": [], "val_f1": [], "train_cm": [], "val_cm": []}

    for epoch in range(num_epochs):
        print(f'Epoch {epoch+1}/{num_epochs} ({model_name})')

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss, running_corrects = 0.0, 0
            all_labels, all_preds = [], []

            for data in dataloaders[phase]:
                # Adapt data unpacking based on model type
                if isinstance(model, FineTunedResNet):
                    image, labels = data  # Unpack for FineTunedResNet
                    image, labels = image.to(device), labels.to(device)
                    left_img = image # Use the single image as input
                    right_img = image # Not actually used by this model
                    #inputs = image
                else:
                    left_img, right_img, labels = data # Unpack for SymmetryResNet
                    left_img, right_img, labels = left_img.to(device), right_img.to(device), labels.to(device)
                    #inputs = (left_img, right_img)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(left_img) if isinstance(model, FineTunedResNet) else model(left_img, right_img)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * left_img.size(0)
                running_corrects += torch.sum(preds == labels.data)

                all_labels.extend(labels.cpu().tolist())
                all_preds.extend(preds.cpu().tolist())

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)

            # Calculate metrics and store the confusion matrix
            acc, precision, recall, f1, cm = calculate_metrics(torch.tensor(all_preds), torch.tensor(all_labels), phase, epoch, model_name)
            metrics[f'{phase}_cm'].append(cm)  # Store confusion matrix

            metrics[f'{phase}_loss'].append(epoch_loss)
            metrics[f'{phase}_acc'].append(epoch_acc.item())
            metrics[f'{phase}_precision'].append(precision)
            metrics[f'{phase}_recall'].append(recall)
            metrics[f'{phase}_f1'].append(f1)


            print(f"{phase.upper()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1: {f1:.4f}")

    return metrics

# Instantiate Models
symmetry_model = SymmetryResNet().to(device)
finetuned_model = FineTunedResNet().to(device)

# Define Loss and Optimizers
criterion = nn.CrossEntropyLoss()
optimizer_symmetry = torch.optim.Adam(symmetry_model.parameters(), lr=0.001)
optimizer_finetuned = torch.optim.Adam(finetuned_model.parameters(), lr=0.001)

# Train and Evaluate Models
symmetry_metrics = train_and_evaluate(symmetry_model, dataloaders_hemispheres, criterion, optimizer_symmetry, num_epochs=10, model_name="SymmetryResNet")
finetuned_metrics = train_and_evaluate(finetuned_model, dataloaders_whole, criterion, optimizer_finetuned, num_epochs=10, model_name="FineTunedResNet")

# Print metrics comparison
print("\nSymmetry Model Metrics:")
print(symmetry_metrics)

print("\nFine-Tuned ResNet Metrics:")
print(finetuned_metrics)

from scipy.stats import ttest_rel

# Convert metrics to DataFrames
sym_df = pd.DataFrame(symmetry_metrics)
ft_df = pd.DataFrame(finetuned_metrics)

# Descriptive Statistics
print("Symmetry Model Descriptive Stats:")
print(sym_df.describe())
print("\nFineTuned ResNet Descriptive Stats:")
print(ft_df.describe())

# Statistical Significance Testing
t_acc, p_acc = ttest_rel(sym_df['val_acc'], ft_df['val_acc'])
t_f1, p_f1 = ttest_rel(sym_df['val_f1'], ft_df['val_f1'])
print(f"\nSymmetry Model Accuracy: t={t_acc:.3f}, p={p_acc:.3f}")
print(f"Symmetry Model F1-Score: t={t_f1:.3f}, p={p_f1:.3f}")

# Plot Training and Validation Curves
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(sym_df['train_acc'], label='Symmetry Model (Train)')
plt.plot(sym_df['val_acc'], label='Symmetry Model (Val)')
plt.plot(ft_df['train_acc'], label='FineTuned ResNet (Train)', linestyle='--')
plt.plot(ft_df['val_acc'], label='FineTuned ResNet (Val)', linestyle='--')
plt.title("Accuracy over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(sym_df['train_f1'], label='Symmetry Model (Train)')
plt.plot(sym_df['val_f1'], label='Symmetry Model (Val)')
plt.plot(ft_df['train_f1'], label='FineTuned ResNet (Train)', linestyle='--')
plt.plot(ft_df['val_f1'], label='FineTuned ResNet (Val)', linestyle='--')
plt.title("F1-Score over Epochs")
plt.xlabel("Epoch")
plt.ylabel("F1-Score")
plt.legend()

plt.tight_layout()
plt.show()

symmetry_cm = symmetry_metrics['val_cm'][9]
finetuned_cm = finetuned_metrics['val_cm'][9]

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
sns.heatmap(symmetry_cm, annot=True, fmt="d", cmap="Blues", ax=axes[0])
axes[0].set_title("Symmetry Model Confusion Matrix")
axes[0].set_xlabel("Predicted")
axes[0].set_ylabel("Actual")

sns.heatmap(finetuned_cm, annot=True, fmt="d", cmap="Greens", ax=axes[1])
axes[1].set_title("FineTuned ResNet Confusion Matrix")
axes[1].set_xlabel("Predicted")
axes[1].set_ylabel("Actual")

plt.tight_layout()
plt.show()

# Testing Phase
symmetry_model.eval()  # Set the model to evaluation mode
all_labels, all_preds = [], []

with torch.no_grad():  # Disable gradient calculations during testing
    for left_img, right_img, labels in test_loader_hemispheres:
        left_img, right_img, labels = left_img.to(device), right_img.to(device), labels.to(device)

        # Adapt input based on model type
        inputs = left_img if isinstance(symmetry_model, FineTunedResNet) else (left_img, right_img)

        outputs = symmetry_model(left_img) if isinstance(symmetry_model, FineTunedResNet) else symmetry_model(left_img, right_img)
        _, preds = torch.max(outputs, 1)

        all_labels.extend(labels.cpu().tolist())
        all_preds.extend(preds.cpu().tolist())

# Calculate and display test metrics
test_acc, test_precision, test_recall, test_f1, test_cm = calculate_metrics(torch.tensor(all_preds), torch.tensor(all_labels), "test", 0, "Model")
print(f"Test Acc: {test_acc:.4f} Precision: {test_precision:.4f} Recall: {test_recall:.4f} F1: {test_f1:.4f}")

print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-Score: {test_f1:.4f}")

# Store the test accuracy for the Symmetry Model
symmetry_model_test_acc = [test_acc]

# Testing Phase for FineTunedResNet
finetuned_model.eval()  # Set the model to evaluation mode
all_labels_ft, all_preds_ft = [], []

with torch.no_grad():  # Disable gradient calculations during testing
    for image, labels in test_loader_whole:  # Use test_loader_whole for FineTunedResNet
        image, labels = image.to(device), labels.to(device)

        outputs = finetuned_model(image)  # Pass image to FineTunedResNet
        _, preds = torch.max(outputs, 1)

        all_labels_ft.extend(labels.cpu().tolist())
        all_preds_ft.extend(preds.cpu().tolist())

# Calculate and display test metrics for FineTunedResNet
test_acc_ft, test_precision_ft, test_recall_ft, test_f1_ft, test_cm_ft = calculate_metrics(torch.tensor(all_preds_ft), torch.tensor(all_labels_ft), "test", 0, "FineTunedResNet")
print(f"FineTunedResNet Test Acc: {test_acc_ft:.4f} Precision: {test_precision_ft:.4f} Recall: {test_recall_ft:.4f} F1: {test_f1_ft:.4f}")

# Store the test accuracy for the FineTuned ResNet
finetuned_model_test_acc = [test_acc_ft]

# Now you can perform the t-test:
t_statistic, p_value = ttest_rel(symmetry_model_test_acc, finetuned_model_test_acc)

# prompt: Paired t-tests to review differences in model accuracy, sensitivity, specificity, false positive rate, and AUC between the SDA and OR methods.
# McNemar’s Test for paired nominal data, to compare the misclassification rates.
# ROC Curve and AUC Comparison to quantify the models' effectiveness in distinguishing between tumor and non-tumor cases and graphically display the trade-offs between sensitivity and specificity.

from statsmodels.stats.contingency_tables import mcnemar
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Initialize all_labels, all_preds, all_preds_ft, and symmetry_probs
all_labels = []
all_preds = []
all_preds_ft = []  # Predictions from FineTunedResNet
symmetry_probs = [] # Probabilities from SymmetryResNet

# Testing Phase (Combine testing for both models here)
symmetry_model.eval() # Call eval() on the actual model
finetuned_model.eval() # Call eval() on the actual model


with torch.no_grad():
    for (image, labels), (left_img, right_img, labels_hemispheres) in zip(test_loader_whole, test_loader_hemispheres):
        # Make sure labels are the same for both datasets
        assert torch.equal(labels, labels_hemispheres), "Labels mismatch between datasets!"

        image, labels = image.to(device), labels.to(device)
        left_img, right_img = left_img.to(device), right_img.to(device)

        # Get predictions and probabilities for SymmetryResNet
        outputs_sym = symmetry_model(left_img, right_img)
        _, preds_sym = torch.max(outputs_sym, 1)
        probs_sym = torch.softmax(outputs_sym, dim=1)  # Get probabilities

        # Get predictions for FineTunedResNet
        outputs_ft = finetuned_model(image)
        _, preds_ft = torch.max(outputs_ft, 1)

        all_labels.extend(labels.cpu().tolist())
        all_preds.extend(preds_sym.cpu().tolist())  # Predictions from SymmetryResNet
        all_preds_ft.extend(preds_ft.cpu().tolist()) # Predictions from FineTunedResNet
        symmetry_probs.extend(probs_sym.cpu().tolist()) # Probabilities from SymmetryResNet

# Create contingency table for McNemar's test
contingency_table = [[0, 0], [0, 0]]
for i in range(len(all_labels)):
    if all_preds[i] == all_labels[i] and all_preds_ft[i] == all_labels[i]:
        contingency_table[0][0] += 1
    elif all_preds[i] != all_labels[i] and all_preds_ft[i] == all_labels[i]:
        contingency_table[0][1] += 1
    elif all_preds[i] == all_labels[i] and all_preds_ft[i] != all_labels[i]:
        contingency_table[1][0] += 1
    elif all_preds[i] != all_labels[i] and all_preds_ft[i] != all_labels[i]:
        contingency_table[1][1] += 1

# Perform McNemar's test
result = mcnemar(contingency_table, exact=False, correction=True)
print(f"McNemar's Test: statistic={result.statistic:.3f}, p-value={result.pvalue:.3f}")

# Calculate ROC curves and AUC scores
fpr_sym, tpr_sym, _ = roc_curve(all_labels, [x[1] for x in symmetry_probs])
roc_auc_sym = auc(fpr_sym, tpr_sym)

# Get probabilities for FineTunedResNet (assuming output is probability at index 1)
finetuned_probs = [x[1] for x in outputs_ft.cpu().tolist()] # Assuming output is already probability # Assuming output is already probability
fpr_ft, tpr_ft, _ = roc_curve(all_labels, finetuned_probs) # Assuming output is already probability # Assuming output is already probability
roc_auc_ft = auc(fpr_ft, tpr_ft)